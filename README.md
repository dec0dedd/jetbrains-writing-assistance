# Improving Writing Assistance at JetBrains AI

This project aims to explore and evaluate existing spell checking tools on various data.

## 1. Data sources

1. Birkbeck Spelling Error Corpus - [Source](https://www.dcs.bbk.ac.uk/~roger/corpora.html)
2. Holbrook Corpus - [Source](https://www.dcs.bbk.ac.uk/~roger/holbrook-tagged.dat)
3. Aspell Testing Corpus - [Source](https://www.dcs.bbk.ac.uk/~roger/aspell.dat)
4. Wikipedia Misspelings Dataset - [Source](https://www.dcs.bbk.ac.uk/~roger/wikipedia.dat)
5. GitHub Typo Corpus - [Source](https://github.com/mhagiwara/github-typo-corpus/tree/master?tab=readme-ov-file)

## 2. Metrics

1. Accuracy
2. 
3. Latency
4. Memory usage

## 3. Tools used for comparison

1. `pyspellchecker` - python spell checker based on Levenshtein distance. [Source](https://github.com/barrust/pyspellchecker?tab=readme-ov-file)
2. `Hunspell` -  an open-source spell checker that supports complex languages and is popular in LibreOffice, OpenOffice, Firefox, and Chrome. It allows custom dictionaries and handles compound words, which makes it suitable for languages with complex morphology. [Source](https://github.com/hunspell/hunspell)
(Note: In order to use it with Python, I've used library `pyhunspell` available [here](https://github.com/pyhunspell/pyhunspell))
3. `NeuSpell` - [Source](https://github.com/neuspell/neuspell)
4. `SymSpell` - [Source](https://github.com/wolfgarbe/SymSpell)
5. `TextBlob` - [Source](https://github.com/sloria/TextBlob)

## 4. Results

- .
- .
- .